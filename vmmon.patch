--- a/vmmon/Makefile
+++ b/vmmon/Makefile
@@ -43,7 +43,11 @@
 endif
 
 
+ifdef KVERSION
+VM_UNAME = $(KVERSION)
+else
 VM_UNAME = $(shell uname -r)
+endif
 
 # Header directory for the running kernel
 ifdef LINUXINCLUDE
@@ -137,7 +141,6 @@
 endif
 
 # Add Spectre options when available
-CC_OPTS += $(call vm_check_gcc,-mindirect-branch=thunk -mindirect-branch-register,)
 
 include $(SRCROOT)/Makefile.kernel
 
--- a/vmmon/include/pgtbl.h
+++ b/vmmon/include/pgtbl.h
@@ -91,7 +91,11 @@
          } else {
             pte_t *pte;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6,5,0)
+            pte = pte_offset_kernel(pmd, addr);
+#else
             pte = pte_offset_map(pmd, addr);
+#endif
             if (pte_present(*pte) == 0) {
                pte_unmap(pte);
                return INVALID_MPN;
--- a/vmmon/include/vm_basic_defs.h
+++ b/vmmon/include/vm_basic_defs.h
@@ -50,7 +50,10 @@
  * References:
  *   C90 7.17, C99 7.19, C11 7.19
  */
-#if !defined(VMKERNEL)
+/* Use linux/stddef.h when building Linux kernel modules. */
+#ifdef KBUILD_MODNAME
+#  include <linux/stddef.h>
+#elif !defined(VMKERNEL)
 #  include <stddef.h>
 #else
    /*
--- a/vmmon/linux/hostif.c
+++ b/vmmon/linux/hostif.c
@@ -47,6 +47,8 @@
 #include <asm/asm.h>
 #include <asm/io.h>
 #include <asm/page.h>
+#include <asm/tlbflush.h>
+#include <asm/irq_vectors.h>
 #include <asm/uaccess.h>
 #include <linux/capability.h>
 #include <linux/kthread.h>
@@ -54,6 +56,7 @@
 #include <linux/hrtimer.h>
 #include <linux/signal.h>
 #include <linux/taskstats_kern.h> // For linux/sched/signal.h without version check
+#include <linux/eventfd.h>
 
 #include "vmware.h"
 #include "x86apic.h"
@@ -80,6 +83,13 @@
 #error CONFIG_HIGH_RES_TIMERS required for acceptable performance
 #endif
 
+/* task's state is read-once rather than volatile from 5.14-rc2. */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0) || defined(get_current_state)
+#define get_task_state(task) READ_ONCE((task)->__state)
+#else
+#define get_task_state(task) ((task)->state)
+#endif
+
 /*
  * Although this is not really related to kernel-compatibility, I put this
  * helper macro here for now for a lack of better place --hpreg
@@ -463,7 +473,7 @@
       ASSERT(vcpuid < vm->numVCPUs);
       t = vm->vmhost->vcpuSemaTask[vcpuid];
       VCPUSet_Remove(&req, vcpuid);
-      if (t && (t->state & TASK_INTERRUPTIBLE)) {
+      if (t && (get_task_state(t) & TASK_INTERRUPTIBLE)) {
          wake_up_process(t);
       }
    }
@@ -614,6 +617,15 @@
    MutexUnlock(&fastClockMutex, callerID);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 8, 0)
+static int crosspage_set_exec(pte_t *pte, unsigned long addr, void *data)
+{
+	struct page *p = data;
+
+	set_pte(pte, mk_pte(p, VM_PAGE_KERNEL_EXEC));
+	return 0;
+}
+#endif
 
 /*
  *----------------------------------------------------------------------
@@ -634,7 +646,29 @@
 static void *
 MapCrossPage(struct page *p)  // IN:
 {
+#if COMPAT_LINUX_VERSION_CHECK_LT(5, 8, 0)
    return vmap(&p, 1, VM_MAP, VM_PAGE_KERNEL_EXEC);
+#else
+   void *addr;
+
+   addr = vmap(&p, 1, VM_MAP, VM_PAGE_KERNEL_EXEC);
+   if (!addr)
+	   return NULL;
+
+   /* Starting with 5.8, vmap() always sets the NX bit, but the cross
+    * page needs to be executable. */
+   if (apply_to_page_range(current->mm, (unsigned long)addr, PAGE_SIZE,
+			   crosspage_set_exec, p)) {
+      vunmap(addr);
+      return NULL;
+   }
+
+   preempt_disable();
+   __flush_tlb_all();
+   preempt_enable();
+
+   return addr;
+#endif
 }
 
 
@@ -1739,7 +1773,10 @@
    lockedPages += global_page_state(NR_PAGETABLE);
 #endif
    /* NR_SLAB_* moved from zone to node in 4.13. */
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 13, 0)
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 9, 0)
+   lockedPages += global_node_page_state_pages(NR_SLAB_UNRECLAIMABLE_B);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(4, 13, 0)
    lockedPages += global_node_page_state(NR_SLAB_UNRECLAIMABLE);
 #else
    lockedPages += global_page_state(NR_SLAB_UNRECLAIMABLE);
@@ -2389,16 +2426,26 @@
 static Bool
 isVAReadable(VA r)  // IN:
 {
-   mm_segment_t old_fs;
    uint32 dummy;
    int ret;
 
+#if defined(HAVE_GET_KERNEL_NOFAULT) || LINUX_VERSION_CODE >= KERNEL_VERSION(5, 17, 0)
+   /*
+    * Exists from 5.10, first indicated by HAVE_GET_KERNEL_NOFAULT,
+    * and from post-5.17 just existing everywhere.
+    */
+   ret = get_kernel_nofault(dummy, (void *)r);
+#else
+   {
+   mm_segment_t old_fs;
+ 
    old_fs = get_fs();
    set_fs(KERNEL_DS);
    r = APICR_TO_ADDR(r, APICR_VERSION);
    ret = HostIF_CopyFromUser(&dummy, r, sizeof dummy);
    set_fs(old_fs);
-
+   }
+#endif
    return ret == 0;
 }
 #endif
@@ -2553,7 +2596,6 @@
                      uint64 *args)   // IN:
 {
    struct file *file;
-   mm_segment_t old_fs;
    int res;
    int waitFD = args[0];
    int timeoutms = args[2];
@@ -2566,22 +2610,19 @@
       return MX_WAITERROR;
    }
 
-   old_fs = get_fs();
-   set_fs(KERNEL_DS);
-
    {
       struct poll_wqueues table;
       unsigned int mask;
 
       poll_initwait(&table);
-      current->state = TASK_INTERRUPTIBLE;
+      __set_current_state(TASK_INTERRUPTIBLE);
       mask = file->f_op->poll(file, &table.pt);
       if (!(mask & (POLLIN | POLLERR | POLLHUP))) {
          vm->vmhost->vcpuSemaTask[vcpuid] = current;
          schedule_timeout(timeoutms * HZ / 1000);  // convert to Hz
          vm->vmhost->vcpuSemaTask[vcpuid] = NULL;
       }
-      current->state = TASK_RUNNING;
+      __set_current_state(TASK_RUNNING);
       poll_freewait(&table);
    }
 
@@ -2590,9 +2631,11 @@
     * the code to happily deal with a pipe or an eventfd. We only care about
     * reading no bytes (EAGAIN - non blocking fd) or sizeof(uint64).
     */
-
-   res = file->f_op->read(file, (char *) &value, sizeof value, &file->f_pos);
-
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+   res = kernel_read(file, file->f_pos, (char *)&value, sizeof value);
+#else
+   res = kernel_read(file, &value, sizeof value, &file->f_pos);
+#endif
    if (res == sizeof value) {
       res = MX_WAITNORMAL;
    } else {
@@ -2601,7 +2644,6 @@
       }
    }
 
-   set_fs(old_fs);
    fput(file);
 
    /*
@@ -2658,7 +2705,7 @@
        */
       struct task_struct *t =
          (struct task_struct *)xchg(&vm->vmhost->vcpuSemaTask[vcpuid], NULL);
-      if (t && (t->state & TASK_INTERRUPTIBLE)) {
+      if (t && (get_task_state(t) & TASK_INTERRUPTIBLE)) {
          wake_up_process(t);
       }
    } ROF_EACH_VCPU_IN_SET_WITH_MAX();
@@ -2688,8 +2730,8 @@
 int
 HostIF_SemaphoreSignal(uint64 *args)  // IN:
 {
+   struct eventfd_ctx *eventfd;
    struct file *file;
-   mm_segment_t old_fs;
    int res;
    int signalFD = args[1];
    uint64 value = 1;  // make an eventfd happy should it be there
@@ -2699,22 +2741,32 @@
       return MX_WAITERROR;
    }
 
-   old_fs = get_fs();
-   set_fs(KERNEL_DS);
+   /*
+    * If it's eventfd, use specific eventfd interface as kernel writes
+    * to eventfd may not be allowed in kernel 5.10 and later.
+    */
+   eventfd = eventfd_ctx_fileget(file);
+   if (!IS_ERR(eventfd)) {
+      eventfd_signal(eventfd, 1);
+      fput(file);
+      return MX_WAITNORMAL;
+   }
 
    /*
     * Always write sizeof(uint64) bytes. This works fine for eventfd and
     * pipes. The data written is formatted to make an eventfd happy should
     * it be present.
     */
-
-   res = file->f_op->write(file, (char *) &value, sizeof value, &file->f_pos);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+   res = kernel_write(file, (char *)&value, sizeof value, file->f_pos);
+#else
+   res = kernel_write(file, &value, sizeof value, &file->f_pos);
+#endif
 
    if (res == sizeof value) {
       res = MX_WAITNORMAL;
    }
 
-   set_fs(old_fs);
    fput(file);
 
    /*
@@ -3261,12 +3313,9 @@
 HostIFFastClockThread(void *unused)  // IN:
 {
    int res;
-   mm_segment_t oldFS;
    unsigned int rate = 0;
    unsigned int prevRate = 0;
 
-   oldFS = get_fs();
-   set_fs(KERNEL_DS);
    allow_signal(SIGKILL);
 
    while ((rate = linuxState.fastClockRate) > MIN_RATE) {
@@ -3289,8 +3338,6 @@
    }
 
  out:
-   set_fs(oldFS);
-
    /*
     * Do not exit thread until we are told to do so.
     */
From 4c2a103fd2d71f2084f1fe7ceacb816b9832ffa2 Mon Sep 17 00:00:00 2001
From: Michal Kubecek <mkubecek@suse.cz>
Date: Sun, 22 Oct 2023 23:24:05 +0200
Subject: [PATCH] vmmon: use get_user_pages to get page PFN

As a side effect of mainline commit 0d940a9b270b ("mm/pgtable: allow
pte_offset_map[_lock]() to fail") in 6.5-rc1, __pte_offset_map(), called by
pte_offset_map(), is no longer exported. WMware developers decided to hack
around this by replacing pte_offset_map() by pte_offset_kernel() which does
not seem to be a good idea and apparently may trigger warn checks in RCU
code on some systems as mentioned in the discussion on issue #223.
Therefore let's use the same solution as we had for 17.0.2 and older
versions as it does not show these problems.

Based on an upstream IRC discussion and the hva_to_pfn_*() family of
functions in KVM code, what PgtblVa2MPNLocked() does seems to be an
incomplete and partial open coded logic of get_user_pages() and as it is
only used to get PFN from a virtual address, it can be easily implemented
using get_user_pages() family.

Without knowledge what exactly are the PFNs used for in VMware, it is hard
to guess the right flags, these seem to work and have been tested by
multiple users over last few weeks.

We could likely use get_user_pages() also on older kernels and it might be
actually cleaner and more reliable as existing open coded implementation
does not seem to handle some corner cases but without knowledge of VMware
internals, it will be safer to stick to existing code where possible.
---
 vmmon-only/include/pgtbl.h | 25 +++++++++++++++++++++++++
 1 file changed, 25 insertions(+)

diff --git a/vmmon-only/include/pgtbl.h b/vmmon-only/include/pgtbl.h
index 3f43c62..7eaa49a 100644
--- a/vmmon-only/include/pgtbl.h
+++ b/vmmon-only/include/pgtbl.h
@@ -25,6 +25,7 @@
 #include "compat_pgtable.h"
 #include "compat_spinlock.h"
 #include "compat_page.h"
+#include "compat_version.h"
 
 
 /*
@@ -45,6 +46,7 @@
  *-----------------------------------------------------------------------------
  */
 
+#if COMPAT_LINUX_VERSION_CHECK_LT(6, 5, 0) // only used by PgtblVa2MPN() below
 static INLINE MPN
 PgtblVa2MPNLocked(struct mm_struct *mm, // IN: Mm structure of a process
                   VA addr)              // IN: Address in the virtual address
@@ -110,6 +112,7 @@ PgtblVa2MPNLocked(struct mm_struct *mm, // IN: Mm structure of a process
    }
    return mpn;
 }
+#endif
 
 
 /*
@@ -129,6 +132,8 @@ PgtblVa2MPNLocked(struct mm_struct *mm, // IN: Mm structure of a process
  *-----------------------------------------------------------------------------
  */
 
+#if COMPAT_LINUX_VERSION_CHECK_LT(6, 5, 0)
+
 static INLINE MPN
 PgtblVa2MPN(VA addr)  // IN
 {
@@ -143,4 +148,24 @@ PgtblVa2MPN(VA addr)  // IN
    return mpn;
 }
 
+#else /* COMPAT_LINUX_VERSION_CHECK_LT(6, 5, 0) */
+
+static INLINE MPN
+PgtblVa2MPN(VA addr)  // IN
+{
+   struct page *page;
+   int npages;
+   MPN mpn;
+
+   npages = get_user_pages_unlocked(addr, 1, &page, FOLL_HWPOISON);
+   if (npages != 1)
+	   return INVALID_MPN;
+   mpn = page_to_pfn(page);
+   put_page(page);
+
+   return mpn;
+}
+
+#endif /* COMPAT_LINUX_VERSION_CHECK_LT(6, 5, 0) */
+
 #endif /* __PGTBL_H__ */
